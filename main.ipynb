{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7d10dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import uuid\n",
    "from typing import List\n",
    "\n",
    "# Core libraries\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.schema.document import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "# from langchain_chroma import Chroma\n",
    "from langchain.retrievers.multi_vector import MultiVectorRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0afa9595",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "PDF_FILE = \"AMTAGVI Commercial FAQ Document.pdf\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "eacfeb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step1_partition_pdf():\n",
    "    \"\"\"Step 1: Extract raw elements from PDF\"\"\"\n",
    "    print(\"Step 1: Partitioning PDF...\")\n",
    "    \n",
    "    elements = partition_pdf(\n",
    "        filename=PDF_FILE,\n",
    "        strategy=\"hi_res\",\n",
    "        infer_table_structure=True,\n",
    "        extract_image_block_types=[\"Image\"],\n",
    "        extract_image_block_to_payload=True\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Extracted {len(elements)} raw elements from PDF\")\n",
    "    return elements\n",
    "\n",
    "def step2_chunk_elements(elements):\n",
    "    \"\"\"Step 2: Chunk elements using by_title strategy\"\"\"\n",
    "    print(\"Step 2: Chunking elements...\")\n",
    "    \n",
    "    chunks = chunk_by_title(\n",
    "        elements,\n",
    "        max_characters=3000,\n",
    "        new_after_n_chars=1000,\n",
    "        combine_text_under_n_chars=500\n",
    "    )\n",
    "    \n",
    "    print(f\"‚úÖ Created {len(chunks)} chunks from elements\")\n",
    "    return chunks\n",
    "\n",
    "def step3_categorize_chunks(chunks):\n",
    "    \"\"\"Step 3: Separate chunks into text, tables, and images\"\"\"\n",
    "    print(\"Step 3: Categorizing chunks...\")\n",
    "    \n",
    "    text_chunks = []\n",
    "    table_chunks = []\n",
    "    image_chunks = []\n",
    "    \n",
    "    # Loop through all chunks and categorize them\n",
    "    for chunk in chunks:\n",
    "        chunk_type = str(type(chunk))\n",
    "        \n",
    "        if 'CompositeElement' in chunk_type:\n",
    "            text_chunks.append(chunk)\n",
    "            \n",
    "        elif 'TableChunk' in chunk_type:  # Changed from 'Table' to 'TableChunk'\n",
    "            table_chunks.append(chunk)\n",
    "    \n",
    "    # Extract images from CompositeElement chunks\n",
    "    for chunk in chunks:\n",
    "        if 'CompositeElement' in str(type(chunk)):\n",
    "            # Look inside CompositeElement for original Image elements\n",
    "            orig_elements = getattr(chunk.metadata, 'orig_elements', [])\n",
    "            for orig_element in orig_elements:\n",
    "                if 'Image' in str(type(orig_element)) and hasattr(orig_element.metadata, 'image_base64'):\n",
    "                    image_chunks.append(orig_element)  # Store the original Image element\n",
    "    \n",
    "    print(f\"‚úÖ Found:\")\n",
    "    print(f\"   üìù Text chunks: {len(text_chunks)}\")\n",
    "    print(f\"   üìä Table chunks: {len(table_chunks)}\")\n",
    "    print(f\"   üñºÔ∏è Image chunks: {len(image_chunks)}\")\n",
    "    \n",
    "    return text_chunks, table_chunks, image_chunks\n",
    "\n",
    "def step4_extract_content(text_chunks, table_chunks, image_chunks):\n",
    "    \"\"\"Step 4: Extract text, HTML, and base64 from chunks\"\"\"\n",
    "    print(\"Step 4: Extracting content...\")\n",
    "    \n",
    "    # Extract text content\n",
    "    texts = [chunk.text for chunk in text_chunks]\n",
    "    \n",
    "    # Extract table HTML (updated for TableChunk)\n",
    "    table_htmls = [getattr(chunk.metadata, 'text_as_html', chunk.text) for chunk in table_chunks]\n",
    "    \n",
    "    # Extract image base64 data (these are now original Image elements)\n",
    "    image_base64s = [chunk.metadata.image_base64 for chunk in image_chunks]\n",
    "    \n",
    "    print(f\"‚úÖ Extracted content:\")\n",
    "    print(f\"   üìù Text chunks: {len(texts)}\")\n",
    "    print(f\"   üìä Table HTMLs: {len(table_htmls)}\")\n",
    "    print(f\"   üñºÔ∏è Image base64s: {len(image_base64s)}\")\n",
    "    \n",
    "    return texts, table_htmls, image_base64s\n",
    "\n",
    "def step5_create_summaries(table_htmls, image_base64s):\n",
    "    \"\"\"Step 5: Create summaries for tables and images\"\"\"\n",
    "    print(\"Step 5: Creating summaries...\")\n",
    "    \n",
    "    llm = ChatOpenAI(model=\"gpt-4o\", api_key=OPENAI_API_KEY, temperature=0)\n",
    "    \n",
    "    # Summarize tables\n",
    "    table_summaries = []\n",
    "    for i, table_html in enumerate(table_htmls):\n",
    "        print(f\"   Summarizing table {i+1}...\")\n",
    "        prompt = f\"\"\"\n",
    "        Analyze this table and provide a concise summary:\n",
    "        \n",
    "        {table_html}\n",
    "        \n",
    "        Include key data points, trends, and what type of information it contains.\n",
    "        Keep it concise (100-150 words).\n",
    "        \"\"\"\n",
    "        \n",
    "        try:\n",
    "            response = llm.invoke(prompt)\n",
    "            table_summaries.append(response.content)\n",
    "        except Exception as e:\n",
    "            print(f\"   Error summarizing table {i+1}: {e}\")\n",
    "            table_summaries.append(\"Table summary unavailable\")\n",
    "    \n",
    "    # Summarize images\n",
    "    image_summaries = []\n",
    "    for i, image_base64 in enumerate(image_base64s):\n",
    "        print(f\"   Analyzing image {i+1}...\")\n",
    "        message = HumanMessage(content=[\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"Analyze this image and provide a concise summary. Include key information about charts, tables, text, or visual elements.\"\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}\n",
    "            }\n",
    "        ])\n",
    "        \n",
    "        try:\n",
    "            response = llm.invoke([message])\n",
    "            image_summaries.append(response.content)\n",
    "        except Exception as e:\n",
    "            print(f\"   Error analyzing image {i+1}: {e}\")\n",
    "            image_summaries.append(\"Image analysis unavailable\")\n",
    "    \n",
    "    print(f\"‚úÖ Created:\")\n",
    "    print(f\"   üìä Table summaries: {len(table_summaries)}\")\n",
    "    print(f\"   üñºÔ∏è Image summaries: {len(image_summaries)}\")\n",
    "    \n",
    "    return table_summaries, image_summaries\n",
    "\n",
    "def step6_setup_retriever():\n",
    "    \"\"\"Step 6: Setup MultiVectorRetriever\"\"\"\n",
    "    print(\"Step 6: Setting up MultiVectorRetriever...\")\n",
    "    \n",
    "    # Create vector store for summaries\n",
    "    vectorstore = Chroma(\n",
    "        collection_name=\"pdf_summaries\", \n",
    "        embedding_function=OpenAIEmbeddings(api_key=OPENAI_API_KEY)\n",
    "    )\n",
    "    \n",
    "    # Create document store for original content\n",
    "    docstore = InMemoryStore()\n",
    "    \n",
    "    # Create retriever\n",
    "    retriever = MultiVectorRetriever(\n",
    "        vectorstore=vectorstore,\n",
    "        docstore=docstore,\n",
    "        id_key=\"doc_id\"\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ MultiVectorRetriever setup complete\")\n",
    "    return retriever\n",
    "\n",
    "def step7_add_to_retriever(retriever, texts, table_summaries, table_htmls, image_summaries, image_base64s):\n",
    "    \"\"\"Step 7: Add all content to retriever\"\"\"\n",
    "    print(\"Step 7: Adding content to retriever...\")\n",
    "    \n",
    "    def add_documents(summaries, originals, content_type):\n",
    "        \"\"\"Helper function to add documents to retriever\"\"\"\n",
    "        if not summaries:\n",
    "            return\n",
    "            \n",
    "        doc_ids = [str(uuid.uuid4()) for _ in summaries]\n",
    "        \n",
    "        # Create summary documents for vector search\n",
    "        summary_docs = [\n",
    "            Document(page_content=summary, metadata={\"doc_id\": doc_ids[i], \"type\": content_type})\n",
    "            for i, summary in enumerate(summaries)\n",
    "        ]\n",
    "        \n",
    "        # Add summaries to vector store (these get embedded and searched)\n",
    "        retriever.vectorstore.add_documents(summary_docs)\n",
    "        \n",
    "        # Add original content to doc store (these get returned to LLM)\n",
    "        retriever.docstore.mset(list(zip(doc_ids, originals)))\n",
    "        \n",
    "        print(f\"   ‚úÖ Added {len(summaries)} {content_type} documents\")\n",
    "    \n",
    "    # Add text (summary = original for text)\n",
    "    add_documents(texts, texts, \"text\")\n",
    "    \n",
    "    # Add tables (summary for search, HTML for LLM)\n",
    "    add_documents(table_summaries, table_htmls, \"table\")\n",
    "    \n",
    "    # Add images (summary for search, base64 for LLM)\n",
    "    add_documents(image_summaries, image_base64s, \"image\")\n",
    "    \n",
    "    print(\"‚úÖ All content added to retriever\")\n",
    "\n",
    "def step8_create_qa_chain(retriever):\n",
    "    \"\"\"Step 8: Create question-answering chain\"\"\"\n",
    "    print(\"Step 8: Creating QA chain...\")\n",
    "    \n",
    "    template = \"\"\"Answer the question based on the following context, which can include text, images, and tables:\n",
    "\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "    prompt = ChatPromptTemplate.from_template(template)\n",
    "    model = ChatOpenAI(temperature=0, model=\"gpt-4o\", api_key=OPENAI_API_KEY)\n",
    "    \n",
    "    chain = (\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | model\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "    \n",
    "    print(\"‚úÖ QA chain created\")\n",
    "    return chain\n",
    "\n",
    "def step9_interactive_search(retriever, qa_chain):\n",
    "    \"\"\"Step 9: Interactive search and QA\"\"\"\n",
    "    print(\"Step 9: Starting interactive mode...\")\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"üîç INTERACTIVE MODE\")\n",
    "    print(\"Commands:\")\n",
    "    print(\"  'search: your query' - see what gets retrieved\")\n",
    "    print(\"  'ask: your question' - get AI answer\")\n",
    "    print(\"  'quit' - exit\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"\\nüí¨ Enter command: \").strip()\n",
    "        \n",
    "        if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "            print(\"üëã Goodbye!\")\n",
    "            break\n",
    "        \n",
    "        if not user_input:\n",
    "            continue\n",
    "            \n",
    "        if user_input.startswith('search:'):\n",
    "            query = user_input[7:].strip()\n",
    "            print(f\"\\nüîç Searching for: '{query}'\")\n",
    "            \n",
    "            try:\n",
    "                results = retriever.get_relevant_documents(query, k=3)\n",
    "                print(f\"Found {len(results)} results:\")\n",
    "                \n",
    "                for i, doc in enumerate(results, 1):\n",
    "                    content = str(doc)\n",
    "                    if content.startswith('data:image'):\n",
    "                        print(f\"\\n{i}. üñºÔ∏è Image (base64 data)\")\n",
    "                    elif '<table' in content.lower():\n",
    "                        print(f\"\\n{i}. üìä Table HTML\")\n",
    "                        print(f\"   Preview: {content[:150]}...\")\n",
    "                    else:\n",
    "                        print(f\"\\n{i}. üìù Text\")\n",
    "                        print(f\"   Content: {content[:200]}...\")\n",
    "                        \n",
    "            except Exception as e:\n",
    "                print(f\"Search error: {e}\")\n",
    "                \n",
    "        elif user_input.startswith('ask:'):\n",
    "            question = user_input[4:].strip()\n",
    "            print(f\"\\nü§ñ AI Answer:\")\n",
    "            \n",
    "            try:\n",
    "                answer = qa_chain.invoke(question)\n",
    "                print(answer)\n",
    "            except Exception as e:\n",
    "                print(f\"QA error: {e}\")\n",
    "        else:\n",
    "            print(\"Please use 'search: query' or 'ask: question' format\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "608e3da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: Partitioning PDF...\n",
      "Warning: No languages specified, defaulting to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `max_size` parameter is deprecated and will be removed in v4.26. Please specify in `size['longest_edge'] instead`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Extracted 259 raw elements from PDF\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Partition PDF\n",
    "elements = step1_partition_pdf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b93ef4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbed2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks = step2_chunk_elements(elements)\n",
    "chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16f44ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Categorizing chunks...\n",
      "‚úÖ Found:\n",
      "   üìù Text chunks: 34\n",
      "   üìä Table chunks: 2\n",
      "   üñºÔ∏è Image chunks: 14\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Categorize chunks\n",
    "text_chunks, table_chunks, image_chunks = step3_categorize_chunks(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2f9bd35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4: Extracting content...\n",
      "‚úÖ Extracted content:\n",
      "   üìù Text chunks: 34\n",
      "   üìä Table HTMLs: 2\n",
      "   üñºÔ∏è Image base64s: 14\n"
     ]
    }
   ],
   "source": [
    "texts, table_htmls, image_base64s = step4_extract_content(text_chunks, table_chunks, image_chunks)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd635aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_summaries, image_summaries = step5_create_summaries(table_htmls, image_base64s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694cec71",
   "metadata": {},
   "outputs": [],
   "source": [
    "retriever = step6_setup_retriever()\n",
    "\n",
    "# Step 7: Add to retriever\n",
    "step7_add_to_retriever(retriever, texts, table_summaries, table_htmls, image_summaries, image_base64s)\n",
    "\n",
    "# Step 8: Create QA chain\n",
    "qa_chain = step8_create_qa_chain(retriever)\n",
    "\n",
    "# Step 9: Interactive mode\n",
    "step9_interactive_search(retriever, qa_chain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
